"""
CROWN 4.6 Offline/Sync Resilience Validation Suite

Tests multi-tab synchronization, conflict resolution, offline queue replay,
and zero-data-loss guarantees for Mina Tasks.

Key Requirements:
- Multi-tab sync via BroadcastChannel (<500ms latency)
- Conflict resolution using vector clocks
- Offline queue FIFO replay with zero data loss
- Checksum validation for data integrity
- Network partition recovery

Author: CROWN 4.6 Testing Team
Date: November 19, 2025
"""

import pytest
import json
import time
from playwright.sync_api import Page, BrowserContext, expect


class OfflineSyncValidator:
    """Validates offline/sync resilience metrics and thresholds."""
    
    def __init__(self):
        self.metrics = {
            'sync_latency_ms': [],
            'conflict_resolution_success': [],
            'offline_queue_integrity': [],
            'checksum_validations': []
        }
        
        self.thresholds = {
            'max_sync_latency_ms': 500,  # Multi-tab sync <500ms
            'conflict_resolution_rate': 1.0,  # 100% successful merges
            'data_loss_tolerance': 0.0,  # Zero data loss
            'checksum_match_rate': 1.0  # 100% integrity
        }
    
    def record_sync_latency(self, latency_ms: float):
        """Record multi-tab sync latency."""
        self.metrics['sync_latency_ms'].append(latency_ms)
    
    def record_conflict_resolution(self, success: bool):
        """Record conflict resolution outcome."""
        self.metrics['conflict_resolution_success'].append(1 if success else 0)
    
    def record_queue_integrity(self, expected: int, actual: int):
        """Record offline queue integrity (expected vs actual items)."""
        integrity = actual / expected if expected > 0 else 1.0
        self.metrics['offline_queue_integrity'].append(integrity)
    
    def record_checksum_validation(self, valid: bool):
        """Record checksum validation result."""
        self.metrics['checksum_validations'].append(1 if valid else 0)
    
    def validate_sync_latency(self) -> dict:
        """Validate multi-tab sync meets latency threshold."""
        if not self.metrics['sync_latency_ms']:
            return {'valid': False, 'reason': 'No sync measurements'}
        
        max_latency = max(self.metrics['sync_latency_ms'])
        avg_latency = sum(self.metrics['sync_latency_ms']) / len(self.metrics['sync_latency_ms'])
        
        return {
            'valid': max_latency <= self.thresholds['max_sync_latency_ms'],
            'max_latency_ms': max_latency,
            'avg_latency_ms': avg_latency,
            'threshold_ms': self.thresholds['max_sync_latency_ms']
        }
    
    def validate_conflict_resolution(self) -> dict:
        """Validate conflict resolution success rate."""
        if not self.metrics['conflict_resolution_success']:
            return {'valid': False, 'reason': 'No conflict tests'}
        
        success_rate = sum(self.metrics['conflict_resolution_success']) / len(self.metrics['conflict_resolution_success'])
        
        return {
            'valid': success_rate >= self.thresholds['conflict_resolution_rate'],
            'success_rate': success_rate,
            'threshold': self.thresholds['conflict_resolution_rate']
        }
    
    def validate_data_integrity(self) -> dict:
        """Validate zero data loss guarantee."""
        if not self.metrics['offline_queue_integrity']:
            return {'valid': False, 'reason': 'No integrity tests'}
        
        min_integrity = min(self.metrics['offline_queue_integrity'])
        avg_integrity = sum(self.metrics['offline_queue_integrity']) / len(self.metrics['offline_queue_integrity'])
        
        return {
            'valid': min_integrity >= (1.0 - self.thresholds['data_loss_tolerance']),
            'min_integrity': min_integrity,
            'avg_integrity': avg_integrity,
            'data_loss_tolerance': self.thresholds['data_loss_tolerance']
        }
    
    def generate_report(self) -> dict:
        """Generate comprehensive offline/sync resilience report."""
        return {
            'sync_latency': self.validate_sync_latency(),
            'conflict_resolution': self.validate_conflict_resolution(),
            'data_integrity': self.validate_data_integrity(),
            'metrics': self.metrics
        }


@pytest.fixture
def sync_validator():
    """Fixture providing OfflineSyncValidator instance."""
    return OfflineSyncValidator()


@pytest.fixture
def multi_context(browser):
    """
    Fixture providing multiple browser contexts for multi-tab testing.
    Returns dict with 'primary' and 'secondary' contexts.
    """
    contexts = {
        'primary': browser.new_context(),
        'secondary': browser.new_context()
    }
    
    yield contexts
    
    # Cleanup
    for context in contexts.values():
        context.close()


class TestCROWN46OfflineSyncResilience:
    """CROWN 4.6 Offline/Sync Resilience Test Suite."""
    
    def test_01_multi_tab_broadcast_sync(self, multi_context: dict, sync_validator: OfflineSyncValidator):
        """
        Validate multi-tab synchronization via BroadcastChannel.
        
        CROWN 4.6 Requirement: Multi-tab sync <500ms latency
        
        Test Scenarios:
        1. Create task in Tab 1 ‚Üí appears in Tab 2
        2. Update task in Tab 2 ‚Üí syncs to Tab 1
        3. Delete task in Tab 1 ‚Üí removed from Tab 2
        4. Measure sync latency for all operations
        """
        print("\n" + "="*80)
        print("TEST 01: Multi-Tab BroadcastChannel Sync")
        print("="*80)
        
        # Check if BroadcastChannel API is available
        primary_page = multi_context['primary'].new_page()
        primary_page.goto('http://0.0.0.0:5000')
        primary_page.wait_for_load_state('networkidle')
        
        has_broadcast = primary_page.evaluate("""
            () => {
                return typeof BroadcastChannel !== 'undefined' &&
                       (window.taskStore && window.taskStore.broadcastSync);
            }
        """)
        
        if not has_broadcast:
            pytest.skip("BroadcastChannel sync not detected - requires implementation")
        
        # Open second tab
        secondary_page = multi_context['secondary'].new_page()
        secondary_page.goto('http://0.0.0.0:5000')
        secondary_page.wait_for_load_state('networkidle')
        
        print(f"\n  ‚úì Opened two independent browser contexts (tabs)")
        
        # Test 1: Create task in Tab 1 ‚Üí appears in Tab 2
        print(f"\n  Scenario 1: Create task in Tab 1")
        
        # Record initial task count in Tab 2
        initial_count_tab2 = secondary_page.evaluate("() => document.querySelectorAll('.task-card').length")
        
        # Create task in Tab 1
        task_title = f"Multi-tab test task {int(time.time() * 1000)}"
        sync_start = time.time()
        
        task_input = primary_page.locator('#task-input, input[placeholder*="task"]').first
        task_input.fill(task_title)
        task_input.press('Enter')
        
        # Wait for task to appear in Tab 1
        primary_page.wait_for_selector(f'text="{task_title}"', timeout=2000)
        
        # Wait for sync to Tab 2 and measure latency
        try:
            secondary_page.wait_for_selector(f'text="{task_title}"', timeout=3000)
            sync_latency = (time.time() - sync_start) * 1000
            sync_validator.record_sync_latency(sync_latency)
            
            new_count_tab2 = secondary_page.evaluate("() => document.querySelectorAll('.task-card').length")
            
            print(f"  ‚úì Task created in Tab 1")
            print(f"  ‚úì Task appeared in Tab 2 (latency: {sync_latency:.0f}ms)")
            print(f"  ‚úì Tab 2 task count: {initial_count_tab2} ‚Üí {new_count_tab2}")
            
            if new_count_tab2 <= initial_count_tab2:
                raise AssertionError(f"Task did not sync to Tab 2 (count unchanged: {new_count_tab2})")
            
        except Exception as e:
            raise AssertionError(f"Multi-tab sync failed: {e}")
        
        # Test 2: Update task in Tab 2 ‚Üí syncs to Tab 1
        print(f"\n  Scenario 2: Update task in Tab 2")
        
        sync_start = time.time()
        
        # Click task in Tab 2 to edit
        task_card_tab2 = secondary_page.locator(f'text="{task_title}"').first
        task_card_tab2.click()
        
        # Update title
        updated_title = f"{task_title} [UPDATED]"
        edit_input = secondary_page.locator('.task-edit-input, input[value*="Multi-tab"]').first
        
        if edit_input.count() > 0:
            edit_input.fill(updated_title)
            edit_input.press('Enter')
            
            # Wait for update to sync to Tab 1
            try:
                primary_page.wait_for_selector(f'text="{updated_title}"', timeout=3000)
                sync_latency = (time.time() - sync_start) * 1000
                sync_validator.record_sync_latency(sync_latency)
                
                print(f"  ‚úì Task updated in Tab 2")
                print(f"  ‚úì Update synced to Tab 1 (latency: {sync_latency:.0f}ms)")
            except:
                print(f"  ‚ö†Ô∏è  Task edit UI not available - skipping update sync test")
        else:
            print(f"  ‚ö†Ô∏è  Task edit input not found - skipping update sync test")
        
        # Test 3: Delete task in Tab 1 ‚Üí removed from Tab 2
        print(f"\n  Scenario 3: Delete task in Tab 1")
        
        sync_start = time.time()
        
        # Delete task in Tab 1
        delete_btn = primary_page.locator('.task-delete, [data-action="delete"]').first
        
        if delete_btn.count() > 0:
            before_count_tab2 = secondary_page.evaluate("() => document.querySelectorAll('.task-card').length")
            
            delete_btn.click()
            
            # Confirm deletion if modal appears
            confirm_btn = primary_page.locator('button:has-text("Delete"), button:has-text("Confirm")').first
            if confirm_btn.count() > 0:
                confirm_btn.click()
            
            # Wait for deletion to sync to Tab 2
            time.sleep(0.5)  # Allow sync
            
            after_count_tab2 = secondary_page.evaluate("() => document.querySelectorAll('.task-card').length")
            sync_latency = (time.time() - sync_start) * 1000
            sync_validator.record_sync_latency(sync_latency)
            
            print(f"  ‚úì Task deleted in Tab 1")
            print(f"  ‚úì Deletion synced to Tab 2 (latency: {sync_latency:.0f}ms)")
            print(f"  ‚úì Tab 2 task count: {before_count_tab2} ‚Üí {after_count_tab2}")
        else:
            print(f"  ‚ö†Ô∏è  Delete button not found - skipping delete sync test")
        
        # Validate sync latency
        result = sync_validator.validate_sync_latency()
        
        if not result['valid']:
            raise AssertionError(
                f"Multi-tab sync latency {result['max_latency_ms']:.0f}ms exceeds "
                f"{result['threshold_ms']}ms threshold"
            )
        
        print(f"\n  ‚úÖ PASS: Multi-tab sync within {result['threshold_ms']}ms threshold")
        print(f"  Max latency: {result['max_latency_ms']:.0f}ms")
        print(f"  Avg latency: {result['avg_latency_ms']:.0f}ms")
    
    
    def test_02_conflict_resolution_vector_clocks(self, multi_context: dict, sync_validator: OfflineSyncValidator):
        """
        Validate conflict resolution using vector clocks.
        
        CROWN 4.6 Requirement: 100% successful conflict merges
        
        Test Scenarios:
        1. Concurrent updates from two tabs to same task
        2. Vector clock comparison determines winner
        3. Both changes preserved (merged) or clear winner chosen
        4. No data corruption
        """
        print("\n" + "="*80)
        print("TEST 02: Conflict Resolution with Vector Clocks")
        print("="*80)
        
        primary_page = multi_context['primary'].new_page()
        primary_page.goto('http://0.0.0.0:5000')
        primary_page.wait_for_load_state('networkidle')
        
        # Check if vector clock conflict resolution is available
        has_vector_clocks = primary_page.evaluate("""
            () => {
                return window.taskStore && 
                       window.taskStore.vectorClock &&
                       window.taskStore.resolveConflict;
            }
        """)
        
        if not has_vector_clocks:
            pytest.skip("Vector clock conflict resolution not detected - requires implementation")
        
        secondary_page = multi_context['secondary'].new_page()
        secondary_page.goto('http://0.0.0.0:5000')
        secondary_page.wait_for_load_state('networkidle')
        
        print(f"\n  ‚úì Vector clock system detected")
        
        # Create a task to conflict on
        task_title = f"Conflict test {int(time.time() * 1000)}"
        task_input = primary_page.locator('#task-input, input[placeholder*="task"]').first
        task_input.fill(task_title)
        task_input.press('Enter')
        
        primary_page.wait_for_selector(f'text="{task_title}"', timeout=2000)
        secondary_page.wait_for_selector(f'text="{task_title}"', timeout=3000)
        
        print(f"\n  ‚úì Created task: '{task_title}'")
        
        # Get task ID
        task_id = primary_page.evaluate("""
            (title) => {
                const cards = document.querySelectorAll('.task-card');
                for (const card of cards) {
                    if (card.textContent.includes(title)) {
                        return card.dataset.taskId || card.id;
                    }
                }
                return null;
            }
        """, task_title)
        
        if not task_id:
            pytest.skip("Cannot identify task ID - requires data-task-id attribute")
        
        print(f"  ‚úì Task ID: {task_id}")
        
        # Simulate concurrent updates
        print(f"\n  Scenario: Concurrent updates from both tabs")
        
        # Update 1: Tab 1 changes priority
        update_tab1 = primary_page.evaluate("""
            async (taskId) => {
                if (window.taskStore && window.taskStore.updateTask) {
                    const result = await window.taskStore.updateTask(taskId, {
                        priority: 'high',
                        updated_by: 'tab1'
                    });
                    return {
                        success: true,
                        vector_clock: result?.vector_clock || window.taskStore.vectorClock
                    };
                }
                return { success: false };
            }
        """, task_id)
        
        # Update 2: Tab 2 changes description (nearly simultaneous)
        update_tab2 = secondary_page.evaluate("""
            async (taskId) => {
                if (window.taskStore && window.taskStore.updateTask) {
                    const result = await window.taskStore.updateTask(taskId, {
                        description: 'Updated from tab 2',
                        updated_by: 'tab2'
                    });
                    return {
                        success: true,
                        vector_clock: result?.vector_clock || window.taskStore.vectorClock
                    };
                }
                return { success: false };
            }
        """, task_id)
        
        if not update_tab1['success'] or not update_tab2['success']:
            pytest.skip("Task update API not available - requires window.taskStore.updateTask()")
        
        print(f"  ‚úì Tab 1 updated priority to 'high'")
        print(f"  ‚úì Tab 2 updated description concurrently")
        
        # Wait for conflict resolution
        time.sleep(1.0)
        
        # Check final state in both tabs
        final_state_tab1 = primary_page.evaluate("""
            async (taskId) => {
                if (window.taskStore && window.taskStore.getTask) {
                    return await window.taskStore.getTask(taskId);
                }
                return null;
            }
        """, task_id)
        
        final_state_tab2 = secondary_page.evaluate("""
            async (taskId) => {
                if (window.taskStore && window.taskStore.getTask) {
                    return await window.taskStore.getTask(taskId);
                }
                return null;
            }
        """, task_id)
        
        if not final_state_tab1 or not final_state_tab2:
            pytest.skip("Cannot retrieve final task state - requires window.taskStore.getTask()")
        
        # Validate conflict resolution
        print(f"\n  Final state validation:")
        print(f"  Tab 1 state: {json.dumps(final_state_tab1, indent=2)[:200]}...")
        print(f"  Tab 2 state: {json.dumps(final_state_tab2, indent=2)[:200]}...")
        
        # Both tabs should converge to same state
        if final_state_tab1 != final_state_tab2:
            print(f"  ‚ö†Ô∏è  WARNING: States diverged - eventual consistency may still be in progress")
            sync_validator.record_conflict_resolution(False)
        else:
            print(f"  ‚úì States converged successfully")
            sync_validator.record_conflict_resolution(True)
            
            # Verify both changes were preserved (if merge strategy)
            has_priority = final_state_tab1.get('priority') == 'high'
            has_description = final_state_tab1.get('description') == 'Updated from tab 2'
            
            if has_priority and has_description:
                print(f"  ‚úì Merge strategy: Both changes preserved")
                sync_validator.record_conflict_resolution(True)
            else:
                print(f"  ‚ÑπÔ∏è  Last-write-wins strategy: One change took precedence")
                sync_validator.record_conflict_resolution(True)
        
        result = sync_validator.validate_conflict_resolution()
        
        if not result['valid']:
            raise AssertionError(
                f"Conflict resolution rate {result['success_rate']*100:.0f}% below "
                f"{result['threshold']*100:.0f}% threshold"
            )
        
        print(f"\n  ‚úÖ PASS: Conflict resolution successful")
    
    
    def test_03_offline_queue_replay_zero_loss(self, page: Page, sync_validator: OfflineSyncValidator):
        """
        Validate offline queue FIFO replay with zero data loss.
        
        CROWN 4.6 Requirement: Zero data loss during offline/online transitions
        
        Test Scenarios:
        1. Go offline
        2. Create multiple tasks (queued)
        3. Update existing task (queued)
        4. Go online
        5. Verify all changes replayed in FIFO order
        6. Validate zero data loss
        """
        print("\n" + "="*80)
        print("TEST 03: Offline Queue Replay & Zero Data Loss")
        print("="*80)
        
        page.goto('http://0.0.0.0:5000')
        page.wait_for_load_state('networkidle')
        
        # Check if offline queue is available
        has_offline_queue = page.evaluate("""
            () => {
                return window.taskStore && 
                       window.taskStore.offlineQueue &&
                       window.taskStore.queueOfflineAction;
            }
        """)
        
        if not has_offline_queue:
            pytest.skip("Offline queue not detected - requires implementation")
        
        print(f"\n  ‚úì Offline queue system detected")
        
        # Get initial task count
        initial_count = page.evaluate("() => document.querySelectorAll('.task-card').length")
        print(f"  ‚úì Initial task count: {initial_count}")
        
        # Go offline
        print(f"\n  Step 1: Going offline...")
        page.context.set_offline(True)
        
        # Wait for offline indicator
        time.sleep(0.5)
        
        offline_status = page.evaluate("""
            () => {
                return !navigator.onLine || 
                       (window.taskStore && window.taskStore.isOffline);
            }
        """)
        
        if not offline_status:
            print(f"  ‚ö†Ô∏è  WARNING: Offline mode not reflected in UI")
        else:
            print(f"  ‚úì Offline mode active")
        
        # Create tasks while offline
        print(f"\n  Step 2: Creating tasks while offline...")
        
        offline_tasks = []
        task_input = page.locator('#task-input, input[placeholder*="task"]').first
        
        for i in range(3):
            task_title = f"Offline task {i+1} - {int(time.time() * 1000)}"
            task_input.fill(task_title)
            task_input.press('Enter')
            time.sleep(0.2)
            offline_tasks.append(task_title)
            print(f"  ‚úì Queued: '{task_title}'")
        
        # Check offline queue size
        queue_size = page.evaluate("""
            () => {
                if (window.taskStore && window.taskStore.getOfflineQueueSize) {
                    return window.taskStore.getOfflineQueueSize();
                }
                if (window.taskStore && window.taskStore.offlineQueue) {
                    return window.taskStore.offlineQueue.length;
                }
                return -1;
            }
        """)
        
        print(f"  ‚úì Offline queue size: {queue_size}")
        
        if queue_size < len(offline_tasks):
            print(f"  ‚ö†Ô∏è  WARNING: Queue size {queue_size} < expected {len(offline_tasks)}")
        
        # Go online
        print(f"\n  Step 3: Going back online...")
        page.context.set_offline(False)
        
        # Wait for sync
        time.sleep(2.0)
        
        # Check that all tasks were replayed
        print(f"\n  Step 4: Validating queue replay...")
        
        final_count = page.evaluate("() => document.querySelectorAll('.task-card').length")
        tasks_added = final_count - initial_count
        
        print(f"  ‚úì Final task count: {final_count}")
        print(f"  ‚úì Tasks added: {tasks_added}")
        
        # Validate each offline task was created
        created_count = 0
        for task_title in offline_tasks:
            exists = page.locator(f'text="{task_title}"').count() > 0
            if exists:
                created_count += 1
                print(f"  ‚úì Found: '{task_title}'")
            else:
                print(f"  ‚ùå Missing: '{task_title}'")
        
        # Record integrity
        sync_validator.record_queue_integrity(len(offline_tasks), created_count)
        
        # Validate zero data loss
        result = sync_validator.validate_data_integrity()
        
        if created_count < len(offline_tasks):
            raise AssertionError(
                f"Data loss detected: {len(offline_tasks)} queued, {created_count} created "
                f"({len(offline_tasks) - created_count} lost)"
            )
        
        print(f"\n  ‚úÖ PASS: Zero data loss - all {len(offline_tasks)} offline tasks replayed")
    
    
    def test_04_checksum_data_integrity(self, page: Page, sync_validator: OfflineSyncValidator):
        """
        Validate data integrity using checksums.
        
        CROWN 4.6 Requirement: 100% data integrity validation
        
        Test Scenarios:
        1. Create tasks and verify checksums generated
        2. Retrieve tasks and validate checksums match
        3. Detect any data corruption
        """
        print("\n" + "="*80)
        print("TEST 04: Checksum Data Integrity Validation")
        print("="*80)
        
        page.goto('http://0.0.0.0:5000')
        page.wait_for_load_state('networkidle')
        
        # Check if checksum validation is available
        has_checksums = page.evaluate("""
            () => {
                return window.taskStore && 
                       (window.taskStore.calculateChecksum || window.taskStore.validateChecksum);
            }
        """)
        
        if not has_checksums:
            pytest.skip("Checksum validation not detected - requires implementation")
        
        print(f"\n  ‚úì Checksum system detected")
        
        # Create a task
        task_title = f"Checksum test {int(time.time() * 1000)}"
        task_input = page.locator('#task-input, input[placeholder*="task"]').first
        task_input.fill(task_title)
        task_input.press('Enter')
        
        page.wait_for_selector(f'text="{task_title}"', timeout=2000)
        print(f"\n  ‚úì Created task: '{task_title}'")
        
        # Retrieve task with checksum
        task_data = page.evaluate("""
            async () => {
                const cards = document.querySelectorAll('.task-card');
                const results = [];
                
                for (const card of cards) {
                    const taskId = card.dataset.taskId || card.id;
                    if (taskId && window.taskStore && window.taskStore.getTask) {
                        const task = await window.taskStore.getTask(taskId);
                        if (task) {
                            results.push({
                                id: taskId,
                                title: task.title,
                                checksum: task.checksum || null,
                                data: task
                            });
                        }
                    }
                }
                
                return results;
            }
        """)
        
        print(f"\n  Retrieved {len(task_data)} tasks with checksums")
        
        # Validate checksums
        valid_count = 0
        for task in task_data[:5]:  # Check first 5 tasks
            if task['checksum']:
                # Recalculate checksum
                calculated = page.evaluate("""
                    (taskData) => {
                        if (window.taskStore && window.taskStore.calculateChecksum) {
                            return window.taskStore.calculateChecksum(taskData);
                        }
                        return null;
                    }
                """, task['data'])
                
                if calculated == task['checksum']:
                    valid_count += 1
                    sync_validator.record_checksum_validation(True)
                    print(f"  ‚úì Valid: '{task['title'][:40]}...' (checksum: {task['checksum'][:16]}...)")
                else:
                    sync_validator.record_checksum_validation(False)
                    print(f"  ‚ùå Invalid: '{task['title'][:40]}...' (mismatch)")
            else:
                print(f"  ‚ö†Ô∏è  No checksum: '{task['title'][:40]}...'")
        
        if valid_count == 0:
            pytest.skip("No checksums found or validation API unavailable")
        
        # Validate integrity rate
        if sync_validator.metrics['checksum_validations']:
            integrity_rate = sum(sync_validator.metrics['checksum_validations']) / len(sync_validator.metrics['checksum_validations'])
            
            if integrity_rate < 1.0:
                raise AssertionError(f"Data integrity compromised: {integrity_rate*100:.0f}% checksum match rate")
            
            print(f"\n  ‚úÖ PASS: 100% data integrity ({valid_count}/{valid_count} checksums valid)")
        else:
            pytest.skip("No checksum validations performed")
    
    
    def test_05_network_partition_recovery(self, page: Page, sync_validator: OfflineSyncValidator):
        """
        Validate behavior during network partitions and recovery.
        
        CROWN 4.6 Requirement: Graceful degradation and recovery
        
        Test Scenarios:
        1. Simulate slow network (high latency)
        2. Simulate intermittent connection (flaky network)
        3. Validate graceful degradation
        4. Validate automatic recovery when network restored
        """
        print("\n" + "="*80)
        print("TEST 05: Network Partition & Recovery")
        print("="*80)
        
        page.goto('http://0.0.0.0:5000')
        page.wait_for_load_state('networkidle')
        
        print(f"\n  Scenario 1: Slow network simulation")
        
        # Simulate slow network (not full offline)
        # Note: Playwright doesn't have built-in latency simulation,
        # so we'll test offline/online cycles instead
        
        initial_count = page.evaluate("() => document.querySelectorAll('.task-card').length")
        
        # Cycle offline/online multiple times
        for cycle in range(3):
            print(f"\n  Cycle {cycle + 1}: Offline ‚Üí Online")
            
            # Go offline
            page.context.set_offline(True)
            time.sleep(0.3)
            
            # Create task while offline
            task_title = f"Recovery test {cycle + 1} - {int(time.time() * 1000)}"
            task_input = page.locator('#task-input, input[placeholder*="task"]').first
            task_input.fill(task_title)
            task_input.press('Enter')
            time.sleep(0.2)
            
            # Go back online
            page.context.set_offline(False)
            time.sleep(0.8)
            
            # Verify task appeared
            exists = page.locator(f'text="{task_title}"').count() > 0
            if exists:
                print(f"  ‚úì Recovered: '{task_title}'")
            else:
                print(f"  ‚ùå Lost: '{task_title}'")
        
        final_count = page.evaluate("() => document.querySelectorAll('.task-card').length")
        recovered = final_count - initial_count
        
        print(f"\n  Recovery summary:")
        print(f"  Tasks created during partitions: 3")
        print(f"  Tasks recovered: {recovered}")
        
        if recovered < 3:
            raise AssertionError(f"Network partition recovery incomplete: {recovered}/3 tasks recovered")
        
        print(f"\n  ‚úÖ PASS: Full recovery from network partitions")
    
    
    def test_99_offline_sync_summary(self, sync_validator: OfflineSyncValidator):
        """Generate comprehensive offline/sync resilience report."""
        print("\n" + "="*80)
        print("OFFLINE/SYNC RESILIENCE SUMMARY")
        print("="*80)
        
        report = sync_validator.generate_report()
        
        print(f"\nüìä Multi-Tab Sync Latency:")
        if report['sync_latency'].get('max_latency_ms'):
            print(f"  Max Latency: {report['sync_latency']['max_latency_ms']:.0f}ms")
            print(f"  Avg Latency: {report['sync_latency']['avg_latency_ms']:.0f}ms")
            print(f"  Threshold: {report['sync_latency']['threshold_ms']}ms")
            print(f"  Status: {'‚úÖ PASS' if report['sync_latency']['valid'] else '‚ùå FAIL'}")
        else:
            print(f"  Status: ‚ö†Ô∏è  No measurements")
        
        print(f"\nüîÑ Conflict Resolution:")
        if report['conflict_resolution'].get('success_rate') is not None:
            print(f"  Success Rate: {report['conflict_resolution']['success_rate']*100:.0f}%")
            print(f"  Threshold: {report['conflict_resolution']['threshold']*100:.0f}%")
            print(f"  Status: {'‚úÖ PASS' if report['conflict_resolution']['valid'] else '‚ùå FAIL'}")
        else:
            print(f"  Status: ‚ö†Ô∏è  No tests")
        
        print(f"\nüíæ Data Integrity:")
        if report['data_integrity'].get('min_integrity') is not None:
            print(f"  Min Integrity: {report['data_integrity']['min_integrity']*100:.0f}%")
            print(f"  Avg Integrity: {report['data_integrity']['avg_integrity']*100:.0f}%")
            print(f"  Data Loss Tolerance: {report['data_integrity']['data_loss_tolerance']*100:.0f}%")
            print(f"  Status: {'‚úÖ PASS' if report['data_integrity']['valid'] else '‚ùå FAIL'}")
        else:
            print(f"  Status: ‚ö†Ô∏è  No tests")
        
        # Save report
        report_path = 'tests/results/offline_sync_resilience_report.json'
        import os
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüìÑ Full report saved: {report_path}")
        print(f"\n{'='*80}")
